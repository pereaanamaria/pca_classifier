{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import ntpath\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import AddChannel, Compose, RandRotate90\n",
    "from monai.transforms import Resize, ScaleIntensity, EnsureType\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import Constants\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "constants = Constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LABELS = constants.labels_pkl\n",
    "\n",
    "LOGS = constants.logs\n",
    "if not os.path.exists(LOGS): os.mkdir(LOGS)\n",
    "    \n",
    "BEST_METRICS = constants.best_metrics\n",
    "if not os.path.exists(BEST_METRICS): os.mkdir(BEST_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_patient_data(patient_id, labels):\n",
    "    idx = get_patient_idx(patient_id)\n",
    "    patient_data = {\n",
    "        'data' : {\n",
    "            'ProxID'  : labels.ProxID.iloc[idx],\n",
    "            'ClinSig' : labels.ClinSig.iloc[idx],\n",
    "            'fid'     : labels.fid.iloc[idx],\n",
    "            'pos'     : labels.pos.iloc[idx],\n",
    "            'zone'    : labels.zone.iloc[idx],\n",
    "            'spacing' : labels.spacing.iloc[idx],\n",
    "            'slices'  : labels.slices.iloc[idx] \n",
    "        },\n",
    "        'images' : {\n",
    "            'T2'     : labels.T2.iloc[idx],\n",
    "            'ADC'    : labels.ADC.iloc[idx],\n",
    "            'DWI'    : labels.DWI.iloc[idx],\n",
    "            'KTrans' : labels.KTrans.iloc[idx] \n",
    "        }\n",
    "    }\n",
    "    return patient_data\n",
    "\n",
    "\n",
    "def get_patientID(filename):\n",
    "    return filename[:14]\n",
    "\n",
    "\n",
    "def get_patient_idx(patient_id):\n",
    "    return int(patient_id[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_pickle(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163, 41)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients = list(labels_df.ProxID)\n",
    "\n",
    "TRAIN_TEST_RATIO = constants.split_ratio\n",
    "train_num = int(len(patients) * TRAIN_TEST_RATIO)\n",
    "\n",
    "random.shuffle(patients)\n",
    "\n",
    "train_data, test_data = patients[:train_num], patients[train_num:] \n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesion_summary(ClinSig):\n",
    "    summary = False\n",
    "    for cs in ClinSig:\n",
    "        summary = summary or cs\n",
    "    return summary\n",
    "\n",
    "\n",
    "def get_images_and_labels(data, labels):\n",
    "    images_arr = []\n",
    "    labels_arr = []\n",
    "    num_files_idx = []\n",
    "\n",
    "    for patient_id in data:\n",
    "        patient_data = get_patient_data(patient_id, labels)\n",
    "\n",
    "        label = 0\n",
    "        clin_sig = get_lesion_summary(patient_data['data']['ClinSig'])\n",
    "        if clin_sig: label = 1\n",
    "\n",
    "        for t2 in patient_data['images']['T2']: \n",
    "            images_arr.append(t2)\n",
    "            labels_arr.append(label)\n",
    "        for adc in patient_data['images']['ADC']: \n",
    "            images_arr.append(adc)\n",
    "            labels_arr.append(label)\n",
    "        for dwi in patient_data['images']['DWI']: \n",
    "            images_arr.append(dwi)\n",
    "            labels_arr.append(label)\n",
    "        images_arr.append(patient_data['images']['KTrans'])\n",
    "        labels_arr.append(label)\n",
    "        num_files_idx.append(len(images_arr))\n",
    "    return images_arr, labels_arr, num_files_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1029, 1029, torch.Size([1029, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_arr, labels_arr, num_files_idx = get_images_and_labels(train_data, labels_df)\n",
    "\n",
    "train_labels_arr = np.array(labels_arr)\n",
    "train_labels_arr = torch.nn.functional.one_hot(torch.as_tensor(labels_arr)).float()\n",
    "\n",
    "len(train_images_arr), len(train_labels_arr), train_labels_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESIZE_SIZE = constants.image_resize\n",
    "\n",
    "train_transforms = Compose([\n",
    "    ScaleIntensity(), \n",
    "    AddChannel(), \n",
    "    Resize((RESIZE_SIZE, RESIZE_SIZE, RESIZE_SIZE)), \n",
    "    RandRotate90(), \n",
    "    EnsureType()\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ScaleIntensity(), \n",
    "    AddChannel(), \n",
    "    Resize((RESIZE_SIZE, RESIZE_SIZE, RESIZE_SIZE)), \n",
    "    EnsureType()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([2, 1, 168, 168, 168]) tensor([[1., 0.],\n",
      "        [1., 0.]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = constants.batch_size\n",
    "NUM_WORKERS = constants.num_workers\n",
    "\n",
    "# Define nifti dataset, data loader\n",
    "check_ds = ImageDataset(\n",
    "    image_files=train_images_arr, \n",
    "    labels=train_labels_arr, \n",
    "    transform=train_transforms\n",
    ")\n",
    "check_loader = DataLoader(\n",
    "    check_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VAL_RATIO = constants.split_ratio\n",
    "train_val_num = num_files_idx[int(len(train_data) * TRAIN_VAL_RATIO)] - 1 \n",
    "\n",
    "train_images, val_images = train_images_arr[:train_val_num], train_images_arr[train_val_num:]\n",
    "train_labels, val_labels = train_labels_arr[:train_val_num], train_labels_arr[train_val_num:]\n",
    "\n",
    "# create a training data loader\n",
    "train_ds = ImageDataset(\n",
    "    image_files=train_images, \n",
    "    labels=train_labels, \n",
    "    transform=train_transforms\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "# create a validation data loader\n",
    "val_ds = ImageDataset(\n",
    "    image_files=val_images, \n",
    "    labels=val_labels, \n",
    "    transform=val_transforms\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS, \n",
    "    pin_memory=pin_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network, loss function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20220713_150900\n"
     ]
    }
   ],
   "source": [
    "today = datetime.today()\n",
    "date_format_metric = today.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(date_format_metric)\n",
    "\n",
    "model = constants.model.to(device)\n",
    "loss_function = constants.loss_function\n",
    "optimizer = constants.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = constants.epochs\n",
    "\n",
    "val_interval = 1\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████▉                                                  | 155/414 [01:56<03:13,  1.34it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    MODEL_STATE_DICT = BEST_METRICS / f'{date_format_metric}.pth'\n",
    "    \n",
    "    for batch_data in tqdm(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"Epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "\n",
    "        num_correct = 0.0\n",
    "        metric_count = 0\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(val_images)\n",
    "                value = torch.eq(val_outputs.argmax(dim=1), val_labels.argmax(dim=1))\n",
    "                metric_count += len(value)\n",
    "                num_correct += value.sum().item()\n",
    "\n",
    "        metric = num_correct / metric_count\n",
    "        metric_values.append(metric)\n",
    "\n",
    "        if metric > best_metric:\n",
    "            best_metric = metric\n",
    "            best_metric_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), MODEL_STATE_DICT)\n",
    "            print(\"Saved new best metric model\")\n",
    "\n",
    "        print(f\"Current epoch: {epoch+1} current accuracy: {metric:.4f} \")\n",
    "        print(f\"Best accuracy: {best_metric:.4f} at epoch {best_metric_epoch}\")\n",
    "        writer.add_scalar(\"val_accuracy\", metric, epoch + 1)\n",
    "\n",
    "print(f\"Training completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_arr, labels_arr = get_images_and_labels(test_data, labels_df)\n",
    "\n",
    "test_labels_arr = np.array(labels_arr)\n",
    "test_labels_arr = torch.nn.functional.one_hot(torch.as_tensor(labels_arr)).float()\n",
    "\n",
    "len(test_images_arr), len(test_labels_arr), test_labels_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST = constants.batch_size_test\n",
    "NUM_WORKERS_TEST = constants.num_workers\n",
    "\n",
    "test_ds = ImageDataset(\n",
    "    image_files=test_images_arr, \n",
    "    labels=test_labels_arr, \n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=BATCH_SIZE_TEST, \n",
    "    num_workers=NUM_WORKERS_TEST, \n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_KPI(kpi, output, label):\n",
    "    tp, tn, fp, fn = kpi\n",
    "    if label[0]:\n",
    "        if output[0]: tp += 1\n",
    "        else: fn +=1\n",
    "    else:\n",
    "        if output[0]: fp += 1\n",
    "        else: tn +=1\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def get_metrics(kpi):\n",
    "    tp, tn, fp, fn = kpi\n",
    "    \n",
    "    recall, precision, f1 = -1, -1, -1\n",
    "    accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "    \n",
    "    if tp + fn != 0: \n",
    "        recall = tp / (tp+fn)\n",
    "        \n",
    "    if tp + fp != 0: \n",
    "        precision = tp / (tp+fp)\n",
    "        \n",
    "    if precision > 0 and recall > 0: \n",
    "        f1 = (2*precision*recall) / (precision+recall)\n",
    "    \n",
    "    return accuracy, recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_test = constants.model.to(device)\n",
    "\n",
    "MODEL = BEST_METRICS / f'{date_format_metric}.pth'\n",
    "\n",
    "model_test.load_state_dict(torch.load(MODEL))\n",
    "model_test.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    KPI = 0, 0, 0, 0\n",
    "    i = 0\n",
    "    \n",
    "    for data in tqdm(test_loader):\n",
    "        test_images, test_labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        test_outputs = model_test(test_images).argmax(dim=1)\n",
    "        test_labels = test_labels.argmax(dim=1)\n",
    "        \n",
    "        value = torch.eq(test_outputs, test_labels)\n",
    "        \n",
    "        KPI = calculate_KPI(KPI, test_outputs, test_labels)\n",
    "        i += 1\n",
    "        \n",
    "    print(f'All test data :: (TP, TN, FP, FN) = {KPI}')\n",
    "    \n",
    "    metrics = get_metrics(KPI)\n",
    "    print(\n",
    "        f'Evaluation metrics:\\n'\n",
    "        f'Accuracy  : {metrics[0]}\\n'\n",
    "        f'Recall    : {metrics[1]}\\n'\n",
    "        f'Precision : {metrics[2]}\\n'\n",
    "        f'F1-score  : {metrics[3]}\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
